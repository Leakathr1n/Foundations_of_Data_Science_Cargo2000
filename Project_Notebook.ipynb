{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "ir",
      "display_name": "R"
    },
    "language_info": {
      "name": "R"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Leakathr1n/Foundations_of_Data_Science_Cargo2000/blob/main/Project_Notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Foundations of Data Science project notebook**\n",
        "\n",
        "Team members:  \n",
        "\n",
        "Predictive Question:"
      ],
      "metadata": {
        "id": "-WZmx4tuuysV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Packages needed to run this notebook**\n",
        "We install and import all necessary libaries for this notebook in the following code cell"
      ],
      "metadata": {
        "id": "VqdpWxjjdS9l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "library(dplyr) # needed for basic transformation on the DS\n",
        "# library(readxl) # needed to open and read xlsx-files\n",
        "library(tidyr) # needed for reshaping\n",
        "library(ggplot2) # needed for plotting\n",
        "install.packages(\"GGally\") # for plotting\n",
        "library(GGally) # for plotting\n",
        "install.packages(\"glmnet\")\n",
        "library(glmnet) #for penalized regression\n",
        "install.packages(\"FNN\") #for K nearest neighbors\n",
        "library(FNN)\n",
        "library(stringr) # needed for working on the airport encoding\n",
        "library(purrr)  # needed for working on the airport encoding\n",
        "library(readr) # needed for reading the csv"
      ],
      "metadata": {
        "id": "fnRjFIPtdcDB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0bed91ff-7e28-4caf-b5b3-17f2f020a974",
        "collapsed": true
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Installing package into ‘/usr/local/lib/R/site-library’\n",
            "(as ‘lib’ is unspecified)\n",
            "\n",
            "Installing package into ‘/usr/local/lib/R/site-library’\n",
            "(as ‘lib’ is unspecified)\n",
            "\n",
            "Installing package into ‘/usr/local/lib/R/site-library’\n",
            "(as ‘lib’ is unspecified)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "set.seed(12345) # setting a seed so that we get the same data set split:)"
      ],
      "metadata": {
        "id": "bFRmUkf5zqoA"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1. Downloading and Cleaning the Data**"
      ],
      "metadata": {
        "id": "4O_Avsjcdr3c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "raw_data<- read_csv('https://raw.githubusercontent.com/Leakathr1n/Foundations_of_Data_Science_Cargo2000/refs/heads/main/00%20Raw%20data/c2k_data_comma.csv')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F3aMbMI7nclp",
        "outputId": "be3a8da5-518e-4d1a-c2d0-dfd0d171f899"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[1mRows: \u001b[22m\u001b[34m3943\u001b[39m \u001b[1mColumns: \u001b[22m\u001b[34m98\u001b[39m\n",
            "\u001b[36m──\u001b[39m \u001b[1mColumn specification\u001b[22m \u001b[36m────────────────────────────────────────────────────────\u001b[39m\n",
            "\u001b[1mDelimiter:\u001b[22m \",\"\n",
            "\u001b[31mchr\u001b[39m (72): i1_dep_2_p, i1_dep_2_e, i1_dep_2_place, i1_rcf_2_p, i1_rcf_2_e, i1...\n",
            "\u001b[32mdbl\u001b[39m (26): nr, i1_legid, i1_rcs_p, i1_rcs_e, i1_dep_1_p, i1_dep_1_e, i1_dep_1...\n",
            "\n",
            "\u001b[36mℹ\u001b[39m Use `spec()` to retrieve the full column specification for this data.\n",
            "\u001b[36mℹ\u001b[39m Specify the column types or set `show_col_types = FALSE` to quiet this message.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# system(\"gdown 19zj1uN65nQVoGdFHQxtxrDVxflopCAuQ6AwR-LXQSuA\")\n",
        "# raw_data <- read_excel(\"/content/c2k_data_comma.xlsx\")"
      ],
      "metadata": {
        "id": "pXoHFpqJdt8F"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "head(raw_data)"
      ],
      "metadata": {
        "id": "XCukQ4Xcgae2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "f740f37c-0994-4f0d-98e0-91ee668ba4de"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table class=\"dataframe\">\n",
              "<caption>A tibble: 6 × 98</caption>\n",
              "<thead>\n",
              "\t<tr><th scope=col>nr</th><th scope=col>i1_legid</th><th scope=col>i1_rcs_p</th><th scope=col>i1_rcs_e</th><th scope=col>i1_dep_1_p</th><th scope=col>i1_dep_1_e</th><th scope=col>i1_dep_1_place</th><th scope=col>i1_rcf_1_p</th><th scope=col>i1_rcf_1_e</th><th scope=col>i1_rcf_1_place</th><th scope=col>⋯</th><th scope=col>o_dep_3_p</th><th scope=col>o_dep_3_e</th><th scope=col>o_dep_3_place</th><th scope=col>o_rcf_3_p</th><th scope=col>o_rcf_3_e</th><th scope=col>o_rcf_3_place</th><th scope=col>o_dlv_p</th><th scope=col>o_dlv_e</th><th scope=col>o_hops</th><th scope=col>legs</th></tr>\n",
              "\t<tr><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>⋯</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "\t<tr><td>0</td><td>5182</td><td> 199</td><td> 218</td><td>210</td><td>215</td><td>609</td><td> 935</td><td> 736</td><td>256</td><td>⋯</td><td>?</td><td>?</td><td>?</td><td>?</td><td>?</td><td>?</td><td> 780</td><td> 434</td><td>1</td><td>2</td></tr>\n",
              "\t<tr><td>1</td><td>6523</td><td> 844</td><td> 584</td><td> 90</td><td>297</td><td>700</td><td>1935</td><td>1415</td><td>431</td><td>⋯</td><td>?</td><td>?</td><td>?</td><td>?</td><td>?</td><td>?</td><td>3870</td><td> 445</td><td>1</td><td>2</td></tr>\n",
              "\t<tr><td>2</td><td>5878</td><td>4380</td><td>4119</td><td> 90</td><td>280</td><td>456</td><td> 905</td><td> 547</td><td>700</td><td>⋯</td><td>?</td><td>?</td><td>?</td><td>?</td><td>?</td><td>?</td><td> 550</td><td>1520</td><td>1</td><td>1</td></tr>\n",
              "\t<tr><td>3</td><td>1275</td><td> 759</td><td> 169</td><td>240</td><td>777</td><td>173</td><td> 340</td><td> 577</td><td>349</td><td>⋯</td><td>?</td><td>?</td><td>?</td><td>?</td><td>?</td><td>?</td><td>3780</td><td> 159</td><td>1</td><td>1</td></tr>\n",
              "\t<tr><td>4</td><td>8117</td><td>1597</td><td>1485</td><td>150</td><td>241</td><td>411</td><td> 585</td><td> 612</td><td>128</td><td>⋯</td><td>?</td><td>?</td><td>?</td><td>?</td><td>?</td><td>?</td><td>4140</td><td>4797</td><td>2</td><td>1</td></tr>\n",
              "\t<tr><td>5</td><td>9889</td><td> 181</td><td>  98</td><td>240</td><td>259</td><td>815</td><td> 215</td><td> 223</td><td>128</td><td>⋯</td><td>?</td><td>?</td><td>?</td><td>?</td><td>?</td><td>?</td><td>3960</td><td> 467</td><td>1</td><td>2</td></tr>\n",
              "</tbody>\n",
              "</table>\n"
            ],
            "text/markdown": "\nA tibble: 6 × 98\n\n| nr &lt;dbl&gt; | i1_legid &lt;dbl&gt; | i1_rcs_p &lt;dbl&gt; | i1_rcs_e &lt;dbl&gt; | i1_dep_1_p &lt;dbl&gt; | i1_dep_1_e &lt;dbl&gt; | i1_dep_1_place &lt;dbl&gt; | i1_rcf_1_p &lt;dbl&gt; | i1_rcf_1_e &lt;dbl&gt; | i1_rcf_1_place &lt;dbl&gt; | ⋯ ⋯ | o_dep_3_p &lt;chr&gt; | o_dep_3_e &lt;chr&gt; | o_dep_3_place &lt;chr&gt; | o_rcf_3_p &lt;chr&gt; | o_rcf_3_e &lt;chr&gt; | o_rcf_3_place &lt;chr&gt; | o_dlv_p &lt;dbl&gt; | o_dlv_e &lt;dbl&gt; | o_hops &lt;dbl&gt; | legs &lt;dbl&gt; |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| 0 | 5182 |  199 |  218 | 210 | 215 | 609 |  935 |  736 | 256 | ⋯ | ? | ? | ? | ? | ? | ? |  780 |  434 | 1 | 2 |\n| 1 | 6523 |  844 |  584 |  90 | 297 | 700 | 1935 | 1415 | 431 | ⋯ | ? | ? | ? | ? | ? | ? | 3870 |  445 | 1 | 2 |\n| 2 | 5878 | 4380 | 4119 |  90 | 280 | 456 |  905 |  547 | 700 | ⋯ | ? | ? | ? | ? | ? | ? |  550 | 1520 | 1 | 1 |\n| 3 | 1275 |  759 |  169 | 240 | 777 | 173 |  340 |  577 | 349 | ⋯ | ? | ? | ? | ? | ? | ? | 3780 |  159 | 1 | 1 |\n| 4 | 8117 | 1597 | 1485 | 150 | 241 | 411 |  585 |  612 | 128 | ⋯ | ? | ? | ? | ? | ? | ? | 4140 | 4797 | 2 | 1 |\n| 5 | 9889 |  181 |   98 | 240 | 259 | 815 |  215 |  223 | 128 | ⋯ | ? | ? | ? | ? | ? | ? | 3960 |  467 | 1 | 2 |\n\n",
            "text/latex": "A tibble: 6 × 98\n\\begin{tabular}{lllllllllllllllllllll}\n nr & i1\\_legid & i1\\_rcs\\_p & i1\\_rcs\\_e & i1\\_dep\\_1\\_p & i1\\_dep\\_1\\_e & i1\\_dep\\_1\\_place & i1\\_rcf\\_1\\_p & i1\\_rcf\\_1\\_e & i1\\_rcf\\_1\\_place & ⋯ & o\\_dep\\_3\\_p & o\\_dep\\_3\\_e & o\\_dep\\_3\\_place & o\\_rcf\\_3\\_p & o\\_rcf\\_3\\_e & o\\_rcf\\_3\\_place & o\\_dlv\\_p & o\\_dlv\\_e & o\\_hops & legs\\\\\n <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & ⋯ & <chr> & <chr> & <chr> & <chr> & <chr> & <chr> & <dbl> & <dbl> & <dbl> & <dbl>\\\\\n\\hline\n\t 0 & 5182 &  199 &  218 & 210 & 215 & 609 &  935 &  736 & 256 & ⋯ & ? & ? & ? & ? & ? & ? &  780 &  434 & 1 & 2\\\\\n\t 1 & 6523 &  844 &  584 &  90 & 297 & 700 & 1935 & 1415 & 431 & ⋯ & ? & ? & ? & ? & ? & ? & 3870 &  445 & 1 & 2\\\\\n\t 2 & 5878 & 4380 & 4119 &  90 & 280 & 456 &  905 &  547 & 700 & ⋯ & ? & ? & ? & ? & ? & ? &  550 & 1520 & 1 & 1\\\\\n\t 3 & 1275 &  759 &  169 & 240 & 777 & 173 &  340 &  577 & 349 & ⋯ & ? & ? & ? & ? & ? & ? & 3780 &  159 & 1 & 1\\\\\n\t 4 & 8117 & 1597 & 1485 & 150 & 241 & 411 &  585 &  612 & 128 & ⋯ & ? & ? & ? & ? & ? & ? & 4140 & 4797 & 2 & 1\\\\\n\t 5 & 9889 &  181 &   98 & 240 & 259 & 815 &  215 &  223 & 128 & ⋯ & ? & ? & ? & ? & ? & ? & 3960 &  467 & 1 & 2\\\\\n\\end{tabular}\n",
            "text/plain": [
              "  nr i1_legid i1_rcs_p i1_rcs_e i1_dep_1_p i1_dep_1_e i1_dep_1_place i1_rcf_1_p\n",
              "1 0  5182      199      218     210        215        609             935      \n",
              "2 1  6523      844      584      90        297        700            1935      \n",
              "3 2  5878     4380     4119      90        280        456             905      \n",
              "4 3  1275      759      169     240        777        173             340      \n",
              "5 4  8117     1597     1485     150        241        411             585      \n",
              "6 5  9889      181       98     240        259        815             215      \n",
              "  i1_rcf_1_e i1_rcf_1_place ⋯ o_dep_3_p o_dep_3_e o_dep_3_place o_rcf_3_p\n",
              "1  736       256            ⋯ ?         ?         ?             ?        \n",
              "2 1415       431            ⋯ ?         ?         ?             ?        \n",
              "3  547       700            ⋯ ?         ?         ?             ?        \n",
              "4  577       349            ⋯ ?         ?         ?             ?        \n",
              "5  612       128            ⋯ ?         ?         ?             ?        \n",
              "6  223       128            ⋯ ?         ?         ?             ?        \n",
              "  o_rcf_3_e o_rcf_3_place o_dlv_p o_dlv_e o_hops legs\n",
              "1 ?         ?              780     434    1      2   \n",
              "2 ?         ?             3870     445    1      2   \n",
              "3 ?         ?              550    1520    1      1   \n",
              "4 ?         ?             3780     159    1      1   \n",
              "5 ?         ?             4140    4797    2      1   \n",
              "6 ?         ?             3960     467    1      2   "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Cleaning the data**\n",
        "\n",
        "We are now cleaning our data and store it in clean_data"
      ],
      "metadata": {
        "id": "8NyIjIKIgoB2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# removing the unnecessary last column\n",
        "clean_data <- raw_data[, !names(raw_data) %in% \"...99\"]"
      ],
      "metadata": {
        "id": "txYGPw4Ignny"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# converting all columns into numerical, except for \"factors\" or categorical variables!\n",
        "# this also ensures that all our \"?\" are converted in NA;)\n",
        "\n",
        "clean_data <- clean_data %>%\n",
        "  mutate(across(everything(), as.character)) %>%      # temporarily ensure character so that we can replace all ?\n",
        "  mutate(across(everything(), ~ na_if(., \"?\"))) %>%   # convert \"?\" → NA\n",
        "  mutate(\n",
        "    across(!matches(\"id|leg|place\", ignore.case = TRUE), as.numeric),  # numeric columns\n",
        "    across(matches(\"id|leg <place\", ignore.case = TRUE), as.factor)     # factor columns\n",
        "  )"
      ],
      "metadata": {
        "id": "XeNqPgb4gmma"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "head(clean_data)"
      ],
      "metadata": {
        "id": "3D2P37_CisSc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. Getting an overview of the dataset**"
      ],
      "metadata": {
        "id": "6nlln5oXomk6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's start with a very broad overview\n",
        "# summary(clean_data)\n",
        "\n",
        "# quite a lot of missing values; let's see if we can bring some structure to this;)\n",
        "# quite a lot of outliers"
      ],
      "metadata": {
        "id": "qSN_U9GZkdrO",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# str(clean_data)"
      ],
      "metadata": {
        "id": "8QvbjaKYqTbE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2.1 Analysing the NA's**\n",
        "\n",
        "We have seen that our dataset has quite a lot of missing values. While they were initially encoded as \"?\", when we converted them into numerical values, the question mark was converted into a proper R \"NA\". We will now dive deeper into the missing values and explore if there is a pattern.\n"
      ],
      "metadata": {
        "id": "meW7cXQAjxF5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For convenience, we add an overview of the dataset here. This will also help us explaining the relationship between the missing values.\n",
        "\n",
        "![Alt text](https://raw.githubusercontent.com/Leakathr1n/Foundations_of_Data_Science_Cargo2000/main/99%20Additional%20material/Overview%20of%20dataset.png)"
      ],
      "metadata": {
        "id": "6kM3auf_lRe2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# creating subsets per each leg & outbound\n",
        "subset_i1 <- clean_data[, c(\"nr\", grep(\"^i1\", names(clean_data), value = TRUE))]\n",
        "subset_i2 <- clean_data[, c(\"nr\", grep(\"^i2\", names(clean_data), value = TRUE))]\n",
        "subset_i3 <- clean_data[, c(\"nr\", grep(\"^i3\", names(clean_data), value = TRUE))]\n",
        "subset_o <- clean_data[, c(\"nr\", grep(\"^o\", names(clean_data), value = TRUE))]\n",
        "\n",
        "# head(subset_i1)"
      ],
      "metadata": {
        "id": "J043CcyFn8uV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# setting up the data sets that I want to loop through\n",
        "subset_list <- list(i1 = subset_i1, i2 = subset_i2, i3 = subset_i3, o  = subset_o)\n",
        "\n",
        "# actually analysing the dataset\n",
        "na_summary_all <- purrr::map_dfr(\n",
        "  names(subset_list),\n",
        "  function(name) {\n",
        "    df <- subset_list[[name]]\n",
        "\n",
        "    df %>%\n",
        "      summarise(across(\n",
        "        everything(),\n",
        "        ~ sum(is.na(.)) / n() * 100, #calculate % NA\n",
        "        .names = \"NA_{.col}\"\n",
        "      )) %>%\n",
        "      pivot_longer(\n",
        "        everything(),\n",
        "        names_to = \"Variable\",\n",
        "        values_to = \"Percent_NA\"\n",
        "      ) %>%\n",
        "      mutate(\n",
        "        Variable = sub(\"^NA_\", \"\", Variable),\n",
        "        Percent_Not_NA = 100 - Percent_NA, # calculate not NA\n",
        "        Subset = name\n",
        "      ) %>%\n",
        "      select(Subset, Variable, Percent_NA, Percent_Not_NA)\n",
        "  }\n",
        ")\n",
        "\n",
        "# print(na_summary_all, n=50)\n",
        "\n",
        "#We can see that there is some kind of pattern; just to be entirely sure, let's double-check with code;)"
      ],
      "metadata": {
        "id": "akN6gPZ7o1vw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting a heat-map!\n",
        "\n",
        "na_clean <- na_summary_all %>%\n",
        "  mutate(\n",
        "    Clean_Variable = sub(\"^(i1_|i2_|i3_|o_)\", \"\", Variable)\n",
        "  )\n",
        "\n",
        "my_order <- c(\n",
        "  \"nr\", \"legid\", \"hops\",\n",
        "  \"rcs_e\", \"rcs_p\", \"dlv_e\", \"dlv_p\",\n",
        "  \"dep_1_e\", \"dep_1_p\", \"dep_1_place\", \"rcf_1_e\", \"rcf_1_p\", \"rcf_1_place\",\n",
        "  \"dep_2_e\", \"dep_2_p\", \"dep_2_place\",\"rcf_2_e\", \"rcf_2_p\", \"rcf_2_place\",\n",
        "  \"dep_3_e\", \"dep_3_p\", \"dep_3_place\",\"rcf_3_e\", \"rcf_3_p\", \"rcf_3_place\"\n",
        ")\n",
        "\n",
        "na_clean <- na_clean %>% mutate(Clean_Variable = factor(Clean_Variable, levels = my_order))\n",
        "\n",
        "options(repr.plot.width = 14, repr.plot.height = 10) # plot size;)\n",
        "\n",
        "ggplot(na_clean, aes(y = Clean_Variable, x = Subset, fill = Percent_NA)) +\n",
        "  geom_tile(color = \"black\") +\n",
        "  geom_text(aes(label = round(Percent_NA, 1))) +\n",
        "  scale_fill_gradient(low = \"white\", high = \"cyan2\", name =\"% Missing\") +\n",
        "  theme_minimal(base_size = 14) +\n",
        "  theme(\n",
        "    axis.text.x = element_text(angle = 45, hjust = 1),\n",
        "    # panel.grid.major = element_line(color = \"black\"),\n",
        "    # panel.grid.minor = element_line(color = \"black\"),\n",
        "    panel.grid = element_blank()\n",
        "  ) +\n",
        "  labs(\n",
        "    title = \"Heatmap of % of Missing Data\",\n",
        "    y = \"Variable\",\n",
        "    x = \"Subset\",\n",
        "    fill = \"% NA\"\n",
        "  )"
      ],
      "metadata": {
        "id": "vavHZzP1bpmE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that there is some pattern with the missing values, i.e. they are NOT missing at random.\n",
        "\n",
        "How to interpret the heat-map now:\n",
        "1. We are NOT missing any unique identifiers, as indicated by the variable \"nr\", legids or hops variables.\n",
        "2. It also seems that we have at least 1 leg per delivery which leads to having no missing values in our first subset /incoming leg and the outgoing leg\n",
        "3. We can see that per hop, we always have the same missing values. E.g. hop 1 in subset i1 contains 0% missing values for all its variables (dep_1_e, dep_1_p, ..., rcf_1_place). The same applies to other hops, and the other subsets. Similarly, there is 99.4% missing values for the 3rd hop in the first incoming leg.\n",
        "\n",
        "Therefore, we can conclude that our NA's are **not missing at random** but that there is a structure to it. We believe that this structure relates to the factor variable hop: if it says one, the transporting leg (i1-i3) only has one delivery, and therefore missing values for hop 2 and hop3. We will verify this claim in the following section:"
      ],
      "metadata": {
        "id": "iPx_53denHP3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a list of subsets with their hops column name\n",
        "subset_list <- list(\n",
        "  i1 = list(data = subset_i1, hops_col = \"i1_hops\"),\n",
        "  i2 = list(data = subset_i2, hops_col = \"i2_hops\"),\n",
        "  i3 = list(data = subset_i3, hops_col = \"i3_hops\"),\n",
        "  o  = list(data = subset_o,  hops_col = \"o_hops\")\n",
        ")\n",
        "\n",
        "hops_summary_all <- data.frame() # Initialise empty data frame to store results\n",
        "\n",
        "# Loop over each subset\n",
        "for(sub_name in names(subset_list)) {\n",
        "  df <- subset_list[[sub_name]]$data\n",
        "  col <- subset_list[[sub_name]]$hops_col\n",
        "\n",
        "  temp <- df %>%\n",
        "    count(.data[[col]]) %>%\n",
        "    mutate(\n",
        "      Percent = n / sum(n) * 100, #calculate per-centage\n",
        "      Subset = sub_name # obtain the subset / leg name\n",
        "    ) %>%\n",
        "    rename(Hops = .data[[col]])\n",
        "\n",
        "  hops_summary_all <- rbind(hops_summary_all, temp)\n",
        "}\n",
        "\n",
        "# Show result\n",
        "hops_summary_all <- hops_summary_all %>% select(Subset, Hops, Percent, n)\n",
        "hops_summary_all"
      ],
      "metadata": {
        "id": "Ntnqm0-frDpO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that those numbers basically confirm our suspicion that the NA's are equivalent to the number of hops as indictated by the hops column!\n",
        "In order to check one by one by row, we did one last check with the following function:"
      ],
      "metadata": {
        "id": "Rekju8qPs40w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "check_hop_consistency <- function(df, hop_var, subset_name) {\n",
        "\n",
        "  df <- df %>% filter(!is.na(.data[[hop_var]]))  # <-- ignore rows with NA in the hops variable\n",
        "\n",
        "  hop1_cols <- grep(\"_1_\", names(df), value = TRUE) # Identify hop-level columns\n",
        "  hop2_cols <- grep(\"_2_\", names(df), value = TRUE)\n",
        "  hop3_cols <- grep(\"_3_\", names(df), value = TRUE)\n",
        "\n",
        "  df %>%\n",
        "    mutate(\n",
        "      subset = subset_name,\n",
        "      hop1_ok = if_else(.data[[hop_var]] >= 1,\n",
        "                        rowSums(!is.na(across(all_of(hop1_cols)))) == length(hop1_cols),\n",
        "                        rowSums(is.na(across(all_of(hop1_cols)))) == length(hop1_cols)),\n",
        "      hop2_ok = if_else(.data[[hop_var]] >= 2,\n",
        "                        rowSums(!is.na(across(all_of(hop2_cols)))) == length(hop2_cols),\n",
        "                        rowSums(is.na(across(all_of(hop2_cols)))) == length(hop2_cols)),\n",
        "      hop3_ok = if_else(.data[[hop_var]] >= 3,\n",
        "                        rowSums(!is.na(across(all_of(hop3_cols)))) == length(hop3_cols),\n",
        "                        rowSums(is.na(across(all_of(hop3_cols)))) == length(hop3_cols)),\n",
        "      all_ok = hop1_ok & hop2_ok & hop3_ok\n",
        "    ) %>%\n",
        "    summarise(\n",
        "      subset = subset_name,\n",
        "      total_rows = n(),\n",
        "      violations = sum(!all_ok),\n",
        "      violation_percent = 100 * violations / total_rows\n",
        "    )\n",
        "}"
      ],
      "metadata": {
        "id": "ZfmtVKiurqT-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result_i1 <- check_hop_consistency(subset_i1, \"i1_hops\", \"i1\") #passing the subsets, and the variable names\n",
        "result_i2 <- check_hop_consistency(subset_i2, \"i2_hops\", \"i2\")\n",
        "result_i3 <- check_hop_consistency(subset_i3, \"i3_hops\", \"i3\")\n",
        "result_o  <- check_hop_consistency(subset_o,  \"o_hops\",  \"o\")\n",
        "\n",
        "bind_rows(result_i1, result_i2, result_i3, result_o)"
      ],
      "metadata": {
        "id": "ClMk4oKbsZGr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is great, we do not have any violations. This means that we have **no missing data at random**. We can therefore proceed to checks on *outliers* and *points with high leverage*."
      ],
      "metadata": {
        "id": "PcW_4Mklt0Fs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Dealing with the NA's**\n",
        "\n",
        "In order to perform prediction, we need to tackle the missing data.\n",
        "There are different ways to deal with missing data, with the easiest one simply dropping rows that contain missing data. However, this would significantly reduce the size of our data-set and will not allow us to actually analyse the different steps in the supply chain.\n",
        "\n",
        "Instead, we have decided to replace missing values simply with a 0. From an economic and business perspective, this makes sense, as those deliveries (since they did not happen) were not unpuctual. Therefore, we can count them as punctual!\n",
        "\n",
        "Please note that we only replace the missing values with 0 after we tranformed the data into our actual variable of interest."
      ],
      "metadata": {
        "id": "ErkCTPlMl7xK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2.2 Look at effective time variable**\n",
        "Before moving to the feature engeneering, it's important to take a look at the distribution of our variables.\\\n",
        "Specifically, we'll take"
      ],
      "metadata": {
        "id": "9APNizuhWojB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2.3. Transformation**\n",
        "\n",
        "Startpoint A:\n",
        "- target: total effective time (maximum of the leg + o)\n",
        "- hops per leg\n",
        "- number of legs\n",
        "- airport ID (place): effective time at the airport/ delay at airport T THAT SPECIFIC STEP (target encoder); no matter in whic leg it occurs; no matter the hop\n",
        "- completely drop the planned time;\n",
        "\n",
        "\n",
        "---------------------------------------------------------\n",
        "just keeping this for completeness now, can delete later!\n",
        "\n",
        "Startpoint B:\n",
        "- target: total delay / total effective time\n",
        "- delay at leg 1, 2, 3\n",
        "- hops (only outgoing)\n",
        "- average delay for the outgoing process\n",
        "- dummy which leg had the largest delay"
      ],
      "metadata": {
        "id": "UrGuLpDu5JkM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3.0 Log Transformation\n",
        "\n",
        "@Francesco will ellaborate more in 2.2"
      ],
      "metadata": {
        "id": "CHvHvzgF-2Ae"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# clean_data <- clean_data %>% mutate(across(ends_with(\"_e\"),log)) # putting all _e variables in log!"
      ],
      "metadata": {
        "id": "eOS06txB3KeL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3.1 Generating our target variable\n",
        "\n",
        "\n",
        "\n",
        "Generating the target variable: effective time as the maximum time between the three legs + outbound time"
      ],
      "metadata": {
        "id": "3BE4ZoiY8Ei6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating the subsets again including the logged variables\n",
        "subset_i1 <- clean_data[, c(\"nr\", grep(\"^i1\", names(clean_data), value = TRUE))]\n",
        "subset_i2 <- clean_data[, c(\"nr\", grep(\"^i2\", names(clean_data), value = TRUE))]\n",
        "subset_i3 <- clean_data[, c(\"nr\", grep(\"^i3\", names(clean_data), value = TRUE))]\n",
        "subset_o <- clean_data[, c(\"nr\", grep(\"^o\", names(clean_data), value = TRUE))]\n",
        "\n",
        "subset_i1$i1_total_e <- rowSums(subset_i1[grep(\"_e$\", names(subset_i1))],na.rm = TRUE)\n",
        "subset_i2$i2_total_e <- rowSums(subset_i2[grep(\"_e$\", names(subset_i2))],na.rm = TRUE)\n",
        "subset_i3$i3_total_e <- rowSums(subset_i3[grep(\"_e$\", names(subset_i3))],na.rm = TRUE)\n",
        "subset_o$o_total_e <- rowSums(subset_o[grep(\"_e$\", names(subset_o))], na.rm = TRUE)"
      ],
      "metadata": {
        "id": "vg9pRYWQ6Cwl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_e <- Reduce(function(x, y) merge(x, y, by = \"nr\", all = TRUE),\n",
        "                  list(subset_i1, subset_i2, subset_i3, subset_o))\n",
        "total_e <- total_e %>% select(i1_total_e,i2_total_e,i3_total_e,o_total_e, nr)\n",
        "head(total_e)"
      ],
      "metadata": {
        "id": "NqvD6hVH9rgs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# find the largest value among the three i columns\n",
        "max_i <- apply(total_e[, c(\"i1_total_e\", \"i2_total_e\", \"i3_total_e\")], 1, max)\n",
        "\n",
        "# add it to o_total_e and store in new column total_e\n",
        "total_e$total_e <- total_e$o_total_e + max_i\n",
        "\n",
        "head(total_e)"
      ],
      "metadata": {
        "id": "3ZwzBhZf9bdg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Distribution of total_e\n",
        "total_e %>%\n",
        "  summarise(\n",
        "    mean  = mean(total_e),\n",
        "    sd    = sd(total_e),\n",
        "    q25   = quantile(total_e, 0.25),\n",
        "    med   = median(total_e),\n",
        "    q75   = quantile(total_e, 0.75),\n",
        "    max   = max(total_e)\n",
        "  )\n",
        "\n",
        "ggplot(total_e, aes(x = total_e)) +\n",
        "  geom_histogram(bins = 30, fill = \"blue\", alpha = 0.7) +\n",
        "  labs(x = \"Total effective time (minutes)\", y = \"Count\")"
      ],
      "metadata": {
        "id": "-fVGDPV1UzSa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The distribution is very skewed, and we have some really big outliers. However, very big delays in supply chain are not uncommon, so it is important to preserve at least a little bit of that information (while at the same time handle their impact on the prediction).\\\n",
        "To do so, we will take a log transformation."
      ],
      "metadata": {
        "id": "RVKiJu4_VMuY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# log-transform\n",
        "total_e <- total_e %>%\n",
        "  mutate(log_total_e = log(total_e))\n",
        "\n",
        "ggplot(total_e, aes(x = log_total_e)) +\n",
        "  geom_histogram(bins = 30, fill = \"blue\", alpha = 0.7) +\n",
        "  labs(x = \"log(Total effective time)\", y = \"Count\")"
      ],
      "metadata": {
        "id": "tSMV9G-yVB7c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The distribution is more bell shaped, while still preserving a bit of the skeweness and the high outliers, justifying using the log-transformation. of out target variable."
      ],
      "metadata": {
        "id": "kZ5BylvKWY4M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "qqplot for sanity check (optional, can discuss with Alessandro if it makes sense including it)"
      ],
      "metadata": {
        "id": "79WL-m5CXqnl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "qqnorm(total_e$total_e); qqline(total_e$total_e, col = \"red\")\n",
        "qqnorm(total_e$log_total_e); qqline(total_e$log_total_e, col = \"red\")"
      ],
      "metadata": {
        "id": "7f59mM27WI7t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Taking a look at the airports\n",
        "Since we want to keep the information about the airports, but OHE for every airport id at every step of the transportation process would be a problem because it would create way too many variables, we want to use target encoding for the airport.\\\n",
        "Before doing it, it's better to do a bit of EDA on the airport variable."
      ],
      "metadata": {
        "id": "ZgaFssVMaAdi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# take logs of the effective time first since we want to target encode for a\n",
        "# log-transformed variable\n",
        "log_clean_data <- clean_data %>% mutate(across(ends_with(\"_e\"),log))\n",
        "head(log_clean_data)"
      ],
      "metadata": {
        "id": "QkUPBHQmbIfC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dep1_long <- log_clean_data %>%\n",
        "  select(\n",
        "    nr,\n",
        "    i1_dep_1_e, i1_dep_1_place,\n",
        "    i2_dep_1_e, i2_dep_1_place,\n",
        "    i3_dep_1_e, i3_dep_1_place,\n",
        "    o_dep_1_e,  o_dep_1_place\n",
        "  ) %>%\n",
        "  pivot_longer(\n",
        "    cols = -nr,\n",
        "    names_to  = c(\"leg\", \".value\"),\n",
        "    names_pattern = \"(i1|i2|i3|o)_dep_1_(e|place)\"\n",
        "  ) %>%\n",
        "  rename(\n",
        "    log_eff_time = e,\n",
        "    airport      = place\n",
        "  ) %>%\n",
        "  filter(!is.na(airport))"
      ],
      "metadata": {
        "id": "oPWE8MjGaqud"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dep1_airport_stats <- dep1_long %>%\n",
        "  group_by(airport) %>%\n",
        "  summarise(\n",
        "    n          = n(),\n",
        "    mean_log_t = mean(log_eff_time),\n",
        "    sd_log_t   = sd(log_eff_time),\n",
        "    .groups = \"drop\"\n",
        "  ) %>%\n",
        "  arrange(desc(n))\n",
        "\n",
        "dep1_airport_stats"
      ],
      "metadata": {
        "id": "HcQgWQiba__A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dep1_by_airport_leg <- dep1_long %>%          # dep1_long from before\n",
        "  group_by(airport, leg) %>%\n",
        "  summarise(\n",
        "    n        = n(),\n",
        "    mean_log_t = mean(log_eff_time),\n",
        "    .groups = \"drop\"\n",
        "  )\n",
        "dep1_by_airport_leg"
      ],
      "metadata": {
        "id": "l-9nSvtfdByf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dep1_by_airport_leg_clean <- dep1_by_airport_leg %>%\n",
        "  mutate(\n",
        "    airport_num = as.numeric(airport)   # \"128\" and \"128.0\" become 128\n",
        "  ) %>%\n",
        "  group_by(airport_num, leg) %>%        # re‑aggregate if needed\n",
        "  summarise(\n",
        "    n          = sum(n),\n",
        "    mean_log_t = weighted.mean(mean_log_t, n),  # or simple mean(...)\n",
        "    .groups = \"drop\"\n",
        "  ) %>%\n",
        "  mutate(airport = factor(airport_num))\n",
        "\n",
        "airport_with_multi_legs <- dep1_by_airport_leg_clean %>%\n",
        "  group_by(airport) %>%\n",
        "  summarise(n_legs = n_distinct(leg), .groups = \"drop\") %>%\n",
        "  filter(n_legs >= 2)\n",
        "\n",
        "dep1_plot_data <- dep1_by_airport_leg_clean %>%\n",
        "  inner_join(airport_with_multi_legs, by = \"airport\")\n",
        "\n",
        "dep1_plot_data %>%\n",
        "  filter(n >= 20) %>%                        # optional: min obs per airport–leg\n",
        "  ggplot(aes(x = airport, y = mean_log_t, fill = leg)) +\n",
        "  geom_col(position = position_dodge(width = 0.7)) +\n",
        "  labs(x = \"Airport id (DEP1)\", y = \"Mean log time\") +\n",
        "  theme_minimal() +\n",
        "  theme(\n",
        "    axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1, size = 6)"
      ],
      "metadata": {
        "id": "cBwtgQKHeNSm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2.3.2 Hops per leg & # legs"
      ],
      "metadata": {
        "id": "DqWB2ngi-mMk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hops_legs <- clean_data %>% select(nr, i1_hops, i2_hops, i3_hops, o_hops, legs)\n",
        "#hops_legs"
      ],
      "metadata": {
        "id": "x6_rL8Zp-ms0",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3.3 Airport ID - Train and Test Split"
      ],
      "metadata": {
        "id": "5LsOvgSSA6Yj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We want to target encode our airport IDs with their average effective time per step in the process. We have airport information given for the steps RCF and DEP for each leg.\n",
        "\n",
        "We specifically keep the split between RCF and DEP as we believe there might be a difference how each airport performs at a different step.\n",
        "\n",
        "Since we need to be careful about **information leakage into our test dataset**, we will already create a train and test split:\n",
        "\n",
        "1. Train: 70\n",
        "2. Test: 30\n",
        "\n",
        "We will now impute the averages across the training data set, and then impose them on the test and training dataset.\n",
        "\n",
        "This means that we will tune our parameters in the following steps using cross-validation, and not validation.\n"
      ],
      "metadata": {
        "id": "zBoqq7ABT9oD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train - Test Split"
      ],
      "metadata": {
        "id": "5Wtr7cgW0JPN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n <- nrow(clean_data) #count the rows in our clean data set\n",
        "train_indices <- sample(1:n, size = 0.7 * n) # selected indices\n",
        "\n",
        "train_data <- clean_data %>% slice(train_indices)\n",
        "test_data <- clean_data %>% slice(-train_indices)\n",
        "\n",
        "print(train_data)\n",
        "print(test_data)"
      ],
      "metadata": {
        "id": "mE10u4TYyemg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Airport ID"
      ],
      "metadata": {
        "id": "_wLzlXkE0P7D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# we are now working off of the train data set to avoid leakage!\n",
        "airports <- train_data %>%\n",
        "  select(\n",
        "    nr,                  # Selects the exact column 'nr'\n",
        "    ends_with(\"_place\"), # Selects columns ending in _place\n",
        "    ends_with(\"_e\")\n",
        "  )\n",
        "\n",
        "print(airports) # verify that we are working with the train data set"
      ],
      "metadata": {
        "id": "1sDwAOk4--Ez"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. RCF"
      ],
      "metadata": {
        "id": "EegvsHslNV7-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rcf <- airports %>% select(grep(\"rcf\", names(airports), value = TRUE)) # only pull rcf values\n",
        "\n",
        "rcf_list <- list() # empty list to store the data sets\n",
        "list_index <- 1 #\n",
        "prefixes <- c(\"i1\", \"i2\", \"i3\", \"o\") # prefixes for selecting the columns\n",
        "\n",
        "for (prefix in prefixes) { # loop through legs\n",
        "  for (hop in 1:3) { # loop through hops\n",
        "    place_col <- paste0(prefix, \"_rcf_\", hop, \"_place\")\n",
        "    e_col <- paste0(prefix, \"_rcf_\", hop, \"_e\")\n",
        "    if (all(c(place_col, e_col) %in% names(rcf))) { # checking both columns exist\n",
        "\n",
        "      # Select the pair, rename the columns for consistency, and filter out NAs\n",
        "      temp_df <- rcf %>%\n",
        "        select(Airport_ID = all_of(place_col), Effective_Time = all_of(e_col)) %>%\n",
        "        filter(!is.na(Airport_ID) & !is.na(Effective_Time)) # Remove rows where either the ID or the Time is missing\n",
        "\n",
        "      # Add the temporary dataset to our list\n",
        "      rcf_list[[list_index]] <- temp_df\n",
        "      list_index <- list_index + 1\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "df_appended_rcf_manual <- bind_rows(rcf_list) # append into one data set!"
      ],
      "metadata": {
        "id": "t9x3qHYgEyj2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# now, let's obtain the mean!\n",
        "airport_summary_stats_rcf <- df_appended_rcf_manual %>%\n",
        "\n",
        "  group_by(Airport_ID) %>%\n",
        "  summarize(\n",
        "    Observation_Count = n(), # Counts how many observations went into the average\n",
        "    Average_Effective_Time = mean(Effective_Time, na.rm = TRUE),\n",
        "    Median_Effective_Time = median(Effective_Time, na.rm = TRUE),\n",
        "    .groups = 'drop'\n",
        "  )\n",
        "\n",
        "head(airport_summary_stats_rcf) # View the resulting summary table"
      ],
      "metadata": {
        "id": "pjMA6gRRMV5W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. DEP"
      ],
      "metadata": {
        "id": "4pT15mBeNbMF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dep <- airports %>% select(grep(\"dep\", names(airports), value = TRUE)) # only pull dep values\n",
        "\n",
        "dep_list <- list() # empty list to store the data sets\n",
        "list_index <- 1 #\n",
        "prefixes <- c(\"i1\", \"i2\", \"i3\", \"o\") # prefixes for selecting the columns\n",
        "\n",
        "for (prefix in prefixes) { # loop through legs\n",
        "  for (hop in 1:3) { # loop through hops\n",
        "    place_col <- paste0(prefix, \"_dep_\", hop, \"_place\")\n",
        "    e_col <- paste0(prefix, \"_dep_\", hop, \"_e\")\n",
        "    if (all(c(place_col, e_col) %in% names(dep))) { # checking both columns exist\n",
        "\n",
        "      # Select the pair, rename the columns for consistency, and filter out NAs\n",
        "      temp_df <- dep %>%\n",
        "        select(Airport_ID = all_of(place_col), Effective_Time = all_of(e_col)) %>%\n",
        "        filter(!is.na(Airport_ID) & !is.na(Effective_Time)) # Remove rows where either the ID or the Time is missing\n",
        "\n",
        "      # Add the temporary dataset to our list\n",
        "      dep_list[[list_index]] <- temp_df\n",
        "      list_index <- list_index + 1\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "df_appended_dep_manual <- bind_rows(dep_list) # append into one data set!"
      ],
      "metadata": {
        "id": "YtOlu9rTNnBL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# now, let's obtain the mean!\n",
        "airport_summary_stats_dep <- df_appended_dep_manual %>%\n",
        "\n",
        "  group_by(Airport_ID) %>%\n",
        "  summarize(\n",
        "    Observation_Count = n(), # Counts how many observations went into the average\n",
        "    Average_Effective_Time = mean(Effective_Time, na.rm = TRUE),\n",
        "    Median_Effective_Time = median(Effective_Time, na.rm = TRUE),\n",
        "    .groups = 'drop'\n",
        "  )\n",
        "\n",
        "head(airport_summary_stats_dep) # View the resulting summary table"
      ],
      "metadata": {
        "id": "yJpQqtOr15D8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# select our variables of interest;)\n",
        "airport_summary_stats_dep <- airport_summary_stats_dep%>%select(Airport_ID, Average_Effective_Time)\n",
        "airport_summary_stats_rcf <- airport_summary_stats_rcf %>%select(Airport_ID, Average_Effective_Time)"
      ],
      "metadata": {
        "id": "xnG9X7sW2Xy1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2.3.4 Bringing new data set together!**"
      ],
      "metadata": {
        "id": "MaVr-efnRPAQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "total_e <- total_e %>%select(total_e, nr) # select variables that we need\n",
        "dataset <- merge(total_e, hops_legs, by = \"nr\")\n",
        "\n",
        "head(dataset) # merge in the airports and replace the airpots with their effective time"
      ],
      "metadata": {
        "id": "IELhLaNTRltP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# replacing the airport IDs with the means calculated on the training data -set\n",
        "\n",
        "airports <- clean_data %>%\n",
        "  select(\n",
        "    nr,                  # Selects the exact column 'nr'\n",
        "    ends_with(\"_place\") # Selects columns ending in _place\n",
        "  )\n",
        "\n",
        "airports_replaced <- airports %>%\n",
        "  mutate(\n",
        "    across(\n",
        "      ends_with(\"_place\"),\n",
        "      ~ {\n",
        "        colname <- cur_column()\n",
        "\n",
        "        # If column contains \"dep\", use airport_summary_stats_dep, else use rcf\n",
        "        lookup <- if (str_detect(colname, \"dep\")) {\n",
        "          airport_summary_stats_dep\n",
        "        } else {\n",
        "          airport_summary_stats_rcf\n",
        "        }\n",
        "\n",
        "        # Join each column of airport IDs to get Average_Effective_Time\n",
        "        lookup_match <- lookup %>%\n",
        "          select(Airport_ID, Average_Effective_Time)\n",
        "\n",
        "        # Replace ID with average time\n",
        "        lookup_match$Average_Effective_Time[match(., lookup_match$Airport_ID)]\n",
        "      }\n",
        "    )\n",
        "  )\n"
      ],
      "metadata": {
        "id": "mdt1vWWjSR5R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset <- dataset %>% left_join(airports_replaced, by = \"nr\")"
      ],
      "metadata": {
        "id": "wrST-32N6v06"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have decided to replace missing values simply with a 0. From an economic and business perspective, this makes sense, as those deliveries (since they did not happen) were not unpuctual. Therefore, we can count them as punctual!"
      ],
      "metadata": {
        "id": "0j0HFp_88BCG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset <- dataset %>% mutate(across(everything(), ~ replace_na(., 0))) # replacing all NA's with 0\n",
        "\n",
        "head(dataset)"
      ],
      "metadata": {
        "id": "u744Y4M_7trL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "write.csv(dataset, file = \"final_dataset.csv\", row.names = FALSE)"
      ],
      "metadata": {
        "id": "POG4aHZoOQSR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**@TEAM:**\n",
        "Here is the train and test data sets! Note that I generate the splits (and the index we use to split) in 2.3.3 Train - Test Split.\n",
        "\n",
        "I then filter the data and only work on the train data for obtaining the average. I then impose this on the full data-set (clean_data). As we keep the same index for splitting into train & test here, there should be no leakage;)"
      ],
      "metadata": {
        "id": "sgd0baSS8Uc4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data <- dataset %>% slice(train_indices)\n",
        "test_data <- dataset %>% slice(-train_indices)"
      ],
      "metadata": {
        "id": "0FxGjrzP8Pk8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verifiying but should be good\n",
        "#head(train_data)\n",
        "#head(test_data)\n",
        "\n",
        "#train_data %>% filter(nr == 0) # perfect;)"
      ],
      "metadata": {
        "id": "kWkG5Wj-_x79"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. Descriptive Stats**\n",
        "Francesco"
      ],
      "metadata": {
        "id": "blZ_ZbxWWQiv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3.1 Correlation**\n",
        "heat map\n",
        "Francesco"
      ],
      "metadata": {
        "id": "kkOGEimXVcYQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4. Predictive task**"
      ],
      "metadata": {
        "id": "WmMDr6F5Vlb-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4.1 Linear Regression**\n",
        "Lea"
      ],
      "metadata": {
        "id": "dQ5sOD1aVswV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4.2 Penalised Regressions**\n",
        "Thibault"
      ],
      "metadata": {
        "id": "aqxSGQPcVwl1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ridge"
      ],
      "metadata": {
        "id": "_9l9ogm_V18U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Lasso"
      ],
      "metadata": {
        "id": "s9nrhs8NV3Xd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Elastic net"
      ],
      "metadata": {
        "id": "_SpZ02gUV5Jk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.3 Non-parametric Models"
      ],
      "metadata": {
        "id": "q6sS2u5nV8QN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###KNN\n",
        "Katherine"
      ],
      "metadata": {
        "id": "jvxYeteHWAA7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Random Forest\n",
        "Katherine"
      ],
      "metadata": {
        "id": "5uMQv7chWJXr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Boosting"
      ],
      "metadata": {
        "id": "m_y9mZ0yWH_7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code Archive"
      ],
      "metadata": {
        "id": "g465MnZTn4d9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# it is a bit annoying with creating loops in google colab since you cannot easily export graphics from this notebook to the google drive :((\n",
        "\n",
        "options(repr.plot.width=30,repr.plot.height=10) # adjusting the plot size so that is better to see in google colab\n",
        "\n",
        "\n",
        "na_summary <- subset_i1 %>%\n",
        "  summarise(across(everything(),\n",
        "                   ~sum(is.na(.))/n() * 100,\n",
        "                   .names = \"NA_{.col}\")) %>%\n",
        "  pivot_longer(everything(),\n",
        "               names_to = \"Variable\",\n",
        "               values_to = \"Percent_NA\") %>%\n",
        "  mutate(Variable = gsub(\"^NA_\", \"\", Variable),\n",
        "         Percent_Not_NA = 100 - Percent_NA) %>%\n",
        "  pivot_longer(cols = c(Percent_NA, Percent_Not_NA),\n",
        "               names_to = \"Type\",\n",
        "               values_to = \"Percent\")\n",
        "\n",
        "na_summary$Type <- recode(na_summary$Type,\n",
        "                          Percent_NA = \"Missing (NA)\", #label cleaning for display\n",
        "                          Percent_Not_NA = \"Not Missing\") # label cleaning for display\n",
        "\n",
        "# actually plotting yeyyy;)\n",
        "ggplot(na_summary, aes(x = reorder(Variable, -Percent), y = Percent, fill = Type)) +\n",
        "  geom_col(position = \"stack\") +\n",
        "  geom_text(aes(label = sprintf(\"%.4f%%\", Percent)), #specifying how many decimals\n",
        "            position = position_stack(vjust = 0.5),\n",
        "            color = \"black\", size = 3.5, fontface = \"bold\") +\n",
        "  labs(\n",
        "    title = \"Percentage of Missing vs Non-Missing Values by Column\",\n",
        "    x = \"Variable\",\n",
        "    y = \"Percentage (%)\"\n",
        "  ) +\n",
        "  # scale_fill_manual(values = c(\"Missing (NA)\" = \"firebrick\", \"Not Missing\" = \"steelblue\")) + #discuss colours we want!\n",
        "  theme_minimal(base_size = 13) +\n",
        "  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n",
        "\n",
        " hops_summary <- subset_i1 %>%\n",
        "    count(i1_hops) %>%\n",
        "    mutate(Percent = n / sum(n) * 100)\n",
        "\n",
        "    print(hops_summary)\n"
      ],
      "metadata": {
        "id": "X6IRpcnpn2-7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Histograms for the variables\n",
        "# Convert to long format\n",
        "long_data <- subset_data %>%\n",
        "  pivot_longer(\n",
        "    cols = everything(),\n",
        "    names_to = \"Variable\",\n",
        "    values_to = \"Value\"\n",
        "  )\n",
        "\n",
        "ggplot(long_data, aes(x = Value)) +\n",
        "  geom_histogram(fill = \"steelblue\", color = \"black\", bins = 30) +\n",
        "  facet_wrap(~Variable, scales = \"free\") +\n",
        "  theme_minimal()"
      ],
      "metadata": {
        "id": "mGIYa7ASwC5O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# identifying outliers based on IQR rules!\n",
        "\n",
        "numeric_df <- subset_final %>% select(where(is.numeric))\n",
        "\n",
        "# Empty list to store results\n",
        "results <- list()\n",
        "\n",
        "# Loop through each numeric variable\n",
        "for (var in names(numeric_df)) {\n",
        "\n",
        "  x <- numeric_df[[var]]\n",
        "\n",
        "  Q1 <- quantile(x, 0.25, na.rm = TRUE)\n",
        "  Q3 <- quantile(x, 0.75, na.rm = TRUE)\n",
        "  IQR_value <- IQR(x, na.rm = TRUE)\n",
        "\n",
        "  lower <- Q1 - 1.5 * IQR_value\n",
        "  upper <- Q3 + 1.5 * IQR_value\n",
        "\n",
        "  idx <- which(x < lower | x > upper)\n",
        "\n",
        "  # Only add if there are outliers\n",
        "  if (length(idx) > 0) {\n",
        "    df_temp <- tibble(\n",
        "      variable = var,\n",
        "      row = idx,\n",
        "      value = x[idx]\n",
        "    )\n",
        "    results[[length(results) + 1]] <- df_temp\n",
        "  }\n",
        "}\n",
        "\n",
        "# Bind all results into one dataframe\n",
        "outliers_df <- bind_rows(results)\n",
        "\n",
        "outliers_df"
      ],
      "metadata": {
        "id": "ej7UGX96C-XP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Outliers and Points with High Leverage**\n",
        "Definition outlier:  \"An outlier is a point for which $y_i$ is far from the value predicted by the model. Outliers can arise for a variety of reasons, such as incorrect recording of an observation during data collection \" (James, Hastie,  Witten, Tibshirani, 2023).\n",
        "\n",
        "We can identify plotting X against Y *or* by plotting the fitted values against the residuals. We can also identify using boxplots or histograms.\n",
        "\n",
        "Definition *high leverage*: \"Points with *high leverage* are observations with an unusual value for $x_i$\"(James, Hastie,  Witten, Tibshirani, 2023). They have a sizable, and often larger impact on the fit than outliers."
      ],
      "metadata": {
        "id": "Ybzc5FIDvjpO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# select columns containing \"rcs\" or \"dvl\" - \"final\" columns\n",
        "cols_final <- names(clean_data) %>%\n",
        "  grep(\"(rcs|dlv)_(p|e)$\", ., value = TRUE) %>% # keep only rcs or dlv columns\n",
        "  setdiff(grep(\"_place\", ., value = TRUE)) # exclude columns containing \"_place\"\n",
        "\n",
        "# Subset the dataset\n",
        "subset_final <- clean_data %>% select(all_of(cols_final),nr)\n",
        "\n",
        "head(subset_final)"
      ],
      "metadata": {
        "id": "KaIvs1-Exvek"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# select columns containing \"rcf or \"dep\" - intermediate steps\n",
        "cols_inter <- names(clean_data) %>%\n",
        "  grep(\"(dep|rcf)_\\\\d+_(p|e)$\", ., value = TRUE) %>% # keep only dep or rcf columns\n",
        "  setdiff(grep(\"_place\", ., value = TRUE)) # exclude columns containing \"_place\"\n",
        "\n",
        "# Subset the dataset\n",
        "subset_inter <- clean_data %>% select(all_of(cols_inter), nr)\n",
        "\n",
        "head(subset_inter)"
      ],
      "metadata": {
        "id": "3u6VeHYPxdZT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_pe_pairs <- function(subset_data) {\n",
        "\n",
        "  options(repr.plot.width = 10, repr.plot.height = 8)\n",
        "  # Identify _p and matching _e columns inside the subset\n",
        "  p_cols <- grep(\"_p$\", names(subset_data), value = TRUE)\n",
        "  e_cols <- gsub(\"_p$\", \"_e\", p_cols)\n",
        "\n",
        "  # Loop over pairs\n",
        "  for(i in seq_along(p_cols)) {\n",
        "\n",
        "    df_pair <- subset_data %>%\n",
        "      select(all_of(c(p_cols[i], e_cols[i]))) %>%\n",
        "      rename(\n",
        "        p = all_of(p_cols[i]),\n",
        "        e = all_of(e_cols[i])\n",
        "      )\n",
        "\n",
        "    g <- ggplot(df_pair, aes(x = p, y = e)) +\n",
        "      geom_point(color = \"steelblue\") +\n",
        "      geom_abline(slope = 1, intercept = 0, linetype = \"dashed\", color = \"red\") +\n",
        "      labs(\n",
        "        title = paste0(\"Scatter plot: \", p_cols[i], \" vs \", e_cols[i]),\n",
        "        x = p_cols[i],\n",
        "        y = e_cols[i]\n",
        "      ) +\n",
        "      theme_minimal()\n",
        "\n",
        "    print(g)\n",
        "  }\n",
        "}"
      ],
      "metadata": {
        "id": "4i3TeASVxKgr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Starting with RCS and DVL"
      ],
      "metadata": {
        "id": "LuiSxl1n_p6u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_pe_pairs(subset_final)"
      ],
      "metadata": {
        "id": "loMT5wrQxNwP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's try and see if we can find a pattern in the outliers\n",
        "\n",
        "Answer: not really"
      ],
      "metadata": {
        "id": "zM6-pJ9sDLHh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# specific outlier analysis based on looking at the scatter plots!\n",
        "\n",
        "outliers_final <- subset_final %>% filter(i2_dlv_e > 40000 | i3_dlv_e > 40000 | o_dlv_e > 40000)\n",
        "\n",
        "head(outliers_final, n=20)"
      ],
      "metadata": {
        "id": "1kJjnegeAKsO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# heatmap to see if there are any correlations between the outliers\n",
        "\n",
        "numeric_df <- outliers_final %>%\n",
        "  select(where(is.numeric))\n",
        "\n",
        "# Compute correlation matrix\n",
        "cor_matrix <- cor(numeric_df, use = \"pairwise.complete.obs\")\n",
        "\n",
        "cor_long <- cor_matrix %>%\n",
        "  as.data.frame() %>%\n",
        "  tibble::rownames_to_column(\"Var1\") %>%\n",
        "  pivot_longer(-Var1, names_to = \"Var2\", values_to = \"Correlation\")\n",
        "\n",
        "ggplot(cor_long, aes(x = Var1, y = Var2, fill = Correlation)) +\n",
        "  geom_tile() +\n",
        "  scale_fill_gradient2(low = \"blue\", mid = \"white\", high = \"red\",\n",
        "                       midpoint = 0, limit = c(-1, 1)) +\n",
        "  theme_minimal() +\n",
        "  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n",
        "  labs(title = \"Correlation Heatmap\",\n",
        "       x = \"\",\n",
        "       y = \"\")"
      ],
      "metadata": {
        "id": "C3d3VHKWBNvU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Continuing with DEP and RCF"
      ],
      "metadata": {
        "id": "4I14nUldt4XW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_pe_pairs(subset_inter)"
      ],
      "metadata": {
        "id": "1al_k3PYyQ4m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transformation: Applying the ln"
      ],
      "metadata": {
        "id": "I1ipBFIVpr87"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "log_modulus <- function(x) {sign(x) * log(abs(x) + 0.1)}"
      ],
      "metadata": {
        "id": "Z1xNqhIOp-kd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Vector of leg prefixes to match\n",
        "leg_prefixes <- c(\"i1\", \"i2\", \"i3\", \"o\")\n",
        "\n",
        "# Select columns matching any of the leg prefixes\n",
        "cols_to_transform <- grep(paste0(\"^((\", paste(leg_prefixes, collapse = \"|\"), \"))_.*delay$\"), names(delay_data), value = TRUE)\n",
        "\n",
        "# Create copies for transformations\n",
        "log_delay_data <- delay_data\n",
        "sqrt_delay_data <- delay_data\n",
        "\n",
        "# Apply log_modulus transform\n",
        "log_delay_data[cols_to_transform] <- lapply(delay_data[cols_to_transform], function(col) log_modulus(col))\n",
        "\n",
        "# Apply ssqrt transform\n",
        "sqrt_delay_data[cols_to_transform] <- lapply(delay_data[cols_to_transform], function(col) ssqrt(col))"
      ],
      "metadata": {
        "id": "ZztDdzXrqHNY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Group prefixes for delays\n",
        "group_prefixes <- c(\"i1_\", \"i2_\", \"i3_\", \"o_\")\n",
        "\n",
        "# Loop through each group, create long format, and plot\n",
        "for (prefix in group_prefixes) {\n",
        "  # Select only delay columns for the current group\n",
        "  group_cols <- grep(paste0(\"^\", prefix, \".*_delay$\"), names(log_delay_data), value = TRUE)\n",
        "\n",
        "  # Pivot longer for this group\n",
        "  group_long <- log_delay_data %>%\n",
        "    select(all_of(group_cols)) %>%\n",
        "    pivot_longer(everything(), names_to = \"segment\", values_to = \"delay\")\n",
        "\n",
        "  # Plot histograms for all delay columns in this group\n",
        "  print(\n",
        "    ggplot(group_long, aes(x = delay)) +\n",
        "      geom_histogram(bins = 30, fill = \"blue\", color = \"black\", alpha = 0.65) +\n",
        "      facet_wrap(~segment, scales = \"free\") +\n",
        "      theme_minimal() +\n",
        "      labs(title = paste0(\"Delay Distributions: \", prefix, \" group\"),\n",
        "           x = \"Delay (log_minutes)\", y = \"Count\")\n",
        "  )\n",
        "}"
      ],
      "metadata": {
        "id": "IygyXsTdqboY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "r5BdusZayA8K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "4wd0vxnjyBb0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.2 Transformation of the data**"
      ],
      "metadata": {
        "id": "mhAw8Dzlv5TI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Startpoint A:\n",
        "- target: total effective time (maximum of the leg + o)\n",
        "- hops per leg\n",
        "- number of legs\n",
        "- airport ID: effective time at the airport/ delay at airport AT THAT SPECIFIC STEP (target encoder); no matter in whic leg it occurs; no matter the hop\n",
        "- completely drop the planned time;\n",
        "\n",
        "Startpoint B:\n",
        "- target: total delay / total effective time\n",
        "- delay at leg 1, 2, 3\n",
        "- hops (only outgoing)\n",
        "- average delay for the outgoing process\n",
        "- dummy which leg had the largest delay\n"
      ],
      "metadata": {
        "id": "2Q9Qljqi1ZpA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.3 Outliers and Points with High Leverage**\n",
        "\n",
        "Definition outlier: \"An outlier is a point for which  yi  is far from the value predicted by the model. Outliers can arise for a variety of reasons, such as incorrect recording of an observation during data collection \" (James, Hastie, Witten, Tibshirani, 2023).\n",
        "\n",
        "We can identify plotting X against Y or by plotting the fitted values against the residuals. We can also identify using boxplots or histograms.\n",
        "\n",
        "Definition high leverage: \"Points with high leverage are observations with an unusual value for  xi \"(James, Hastie, Witten, Tibshirani, 2023). They have a sizable, and often larger impact on the fit than outliers"
      ],
      "metadata": {
        "id": "0kqb_OH_c-yl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Summary stats\n",
        "\n",
        "summary_stats <- delay_data %>%\n",
        "  summarise(across(matches(\"_delay$\"), # Apply to columns ending with '_delay'\n",
        "                   list(mean = ~mean(., na.rm=TRUE),\n",
        "                        median = ~median(., na.rm=TRUE),\n",
        "                        sd = ~sd(., na.rm=TRUE)))) %>%\n",
        "  pivot_longer(cols = everything(),\n",
        "               names_to = c(\"segment\", \".value\"),\n",
        "               names_pattern = \"(.*)_(.*)$\")\n",
        "\n",
        "summary_stats"
      ],
      "metadata": {
        "id": "v2Ozcl-2n3a0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Group prefixes for delays\n",
        "group_prefixes <- c(\"i1_\", \"i2_\", \"i3_\", \"o_\")\n",
        "\n",
        "# Loop through each group, create long format, and plot\n",
        "for (prefix in group_prefixes) {\n",
        "  # Select only delay columns for the current group\n",
        "  group_cols <- grep(paste0(\"^\", prefix, \".*_delay$\"), names(delay_data), value = TRUE)\n",
        "\n",
        "  # Pivot longer for this group\n",
        "  group_long <- delay_data %>%\n",
        "    select(all_of(group_cols)) %>%\n",
        "    pivot_longer(everything(), names_to = \"segment\", values_to = \"delay\")\n",
        "\n",
        "  # Plot histograms for all delay columns in this group\n",
        "  print(\n",
        "    ggplot(group_long, aes(x = delay)) +\n",
        "      geom_histogram(bins = 30, fill = \"blue\", color = \"black\", alpha = 0.65) +\n",
        "      facet_wrap(~segment, scales = \"free\") +\n",
        "      theme_minimal() +\n",
        "      labs(title = paste0(\"Delay Distributions: \", prefix, \" group\"),\n",
        "           x = \"Delay (minutes)\", y = \"Count\")\n",
        "  )\n",
        "}"
      ],
      "metadata": {
        "id": "Vk1XLHV5J7jx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some of the data is really skewed. To handle this, we could drop the outliers or use a transformation. \\\n",
        "IMO transforming the data is better option because my guess is that extreme outliers in this setting are not errors but real delays, which carry important information. \\\n",
        "Since we have negative values, we can't directly apply a log transformation or a sqrt, but we need to use a transformation which preserves negative values. Two options:\n",
        "$$\n",
        "l(x)=sign(x)*log(|x|+0.1)\n",
        "$$\n",
        "(the $0.1$ is added to values equal to 0, if any)\n",
        "Or\n",
        "$$\n",
        "l(x)=sign(x)*sqrt(|x|)\n",
        "$$\n",
        "\n",
        "\n",
        "**LR**:\n",
        "1. should we maybe apply the transformation directly to planned and effective time? Then we can avoid this, but I am open to whatever!\n",
        "2. I think handling outliers should come before we apply any transformation by the way, i.e. before we create the difference variable IMO, but this is just a minor comment and structure, not the actual handling"
      ],
      "metadata": {
        "id": "3LnAs89FLaVM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# creating our square root-function\n",
        "ssqrt <- function(x) {sign(x) * sqrt(abs(x))}"
      ],
      "metadata": {
        "id": "TxnLjyUqZvBc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Vector of leg prefixes to match\n",
        "leg_prefixes <- c(\"i1\", \"i2\", \"i3\", \"o\")\n",
        "\n",
        "# Select columns matching any of the leg prefixes\n",
        "cols_to_transform <- grep(paste0(\"^((\", paste(leg_prefixes, collapse = \"|\"), \"))_.*delay$\"), names(delay_data), value = TRUE)\n",
        "\n",
        "# Create copies for transformations\n",
        "sqrt_delay_data <- delay_data\n",
        "\n",
        "# Apply ssqrt transform\n",
        "sqrt_delay_data[cols_to_transform] <- lapply(delay_data[cols_to_transform], function(col) ssqrt(col))"
      ],
      "metadata": {
        "id": "3za-giPENuKU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "head(sqrt_delay_data)"
      ],
      "metadata": {
        "id": "IVzhg1WQaHfh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Group prefixes for delays\n",
        "group_prefixes <- c(\"i1_\", \"i2_\", \"i3_\", \"o_\")\n",
        "\n",
        "# Loop through each group, create long format, and plot\n",
        "for (prefix in group_prefixes) {\n",
        "  # Select only delay columns for the current group\n",
        "  group_cols <- grep(paste0(\"^\", prefix, \".*_delay$\"), names(sqrt_delay_data), value = TRUE)\n",
        "\n",
        "  # Pivot longer for this group\n",
        "  group_long <- sqrt_delay_data %>%\n",
        "    select(all_of(group_cols)) %>%\n",
        "    pivot_longer(everything(), names_to = \"segment\", values_to = \"delay\")\n",
        "\n",
        "  # Plot histograms for all delay columns in this group\n",
        "  print(\n",
        "    ggplot(group_long, aes(x = delay)) +\n",
        "      geom_histogram(bins = 30, fill = \"blue\", color = \"black\", alpha = 0.65) +\n",
        "      facet_wrap(~segment, scales = \"free\") +\n",
        "      theme_minimal() +\n",
        "      labs(title = paste0(\"Delay Distributions: \", prefix, \" group\"),\n",
        "           x = \"Delay (sqrt_minutes)\", y = \"Count\")\n",
        "  )\n",
        "}"
      ],
      "metadata": {
        "id": "jU8N7cbvOVSJ",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.1 Creating Variable of Interest: Delay of shipments\n",
        "\n",
        "We aim to predict the delay of certain shipments. Our data set only gives us information about **planned** and **effective** duration. In order to predict the **delay**, we need to create this variable."
      ],
      "metadata": {
        "id": "sWwoStlkonRo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get vectors of planned and effective column names (matched)\n",
        "planned_cols <- grep('_p$', names(clean_data), value = TRUE)\n",
        "effective_cols <- grep('_e$', names(clean_data), value = TRUE)\n",
        "\n",
        "for (i in seq_along(planned_cols)) {\n",
        "  p_col <- planned_cols[i]\n",
        "  e_col <- effective_cols[i]\n",
        "  delay_col <- sub('_p$', '_delay', p_col)\n",
        "\n",
        "  # Create delay column\n",
        "  clean_data[[delay_col]] <- clean_data[[e_col]] - clean_data[[p_col]]\n",
        "\n",
        "  # Get current column order\n",
        "  cols <- names(clean_data)\n",
        "\n",
        "  # Find position of effective column\n",
        "  e_pos <- which(cols == e_col)\n",
        "\n",
        "  # Remove delay column from current position\n",
        "  cols <- cols[cols != delay_col]\n",
        "\n",
        "  # Insert delay column after effective column\n",
        "  cols <- append(cols, delay_col, after = e_pos)\n",
        "\n",
        "  # Reorder dataframe columns to this new order\n",
        "  clean_data <- clean_data[, cols]\n",
        "}"
      ],
      "metadata": {
        "id": "hD-zHKNuv1og"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "head(clean_data) # looks correct! :)) # YEYYYY"
      ],
      "metadata": {
        "id": "ynxTTJWnAOik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop all the planned and effective columns\n",
        "delay_data <- clean_data %>% select(-matches(\"_e$\"), -matches(\"_p$\"))"
      ],
      "metadata": {
        "id": "qX0o4G7wBtus"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "head(delay_data)"
      ],
      "metadata": {
        "id": "3YulcdzdDCkA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Replacing NAs by 0 for all the columns ending with \"delay\"\n",
        "delay_data_test <- delay_data %>%\n",
        "  mutate(across(\n",
        "    .cols = c(where(is.numeric), contains(\"place\")), # numeric columns and those that contain a \"place\"\n",
        "    .fns  = ~ replace(.x, is.na(.x), 0)\n",
        "  ))\n",
        "\n",
        "#Creating total_delay for each leg\n",
        "delay_data_test <- delay_data_test %>%\n",
        "  mutate(\n",
        "    i1_total_delay = i1_rcs_delay + i1_dep_1_delay + i1_rcf_1_delay +\n",
        "                     i1_dep_2_delay + i1_rcf_2_delay + i1_dep_3_delay +\n",
        "                     i1_rcf_3_delay + i1_dlv_delay,\n",
        "\n",
        "    i2_total_delay = i2_rcs_delay + i2_dep_1_delay + i2_rcf_1_delay +\n",
        "                     i2_dep_2_delay + i2_rcf_2_delay + i2_dep_3_delay +\n",
        "                     i2_rcf_3_delay + i2_dlv_delay,\n",
        "\n",
        "    i3_total_delay = i3_rcs_delay + i3_dep_1_delay + i3_rcf_1_delay +\n",
        "                     i3_dep_2_delay + i3_rcf_2_delay + i3_dep_3_delay +\n",
        "                     i3_rcf_3_delay + i3_dlv_delay,\n",
        "\n",
        "    o_total_delay  = o_rcs_delay + o_dep_1_delay + o_rcf_1_delay +\n",
        "                     o_dep_2_delay + o_rcf_2_delay + o_dep_3_delay +\n",
        "                     o_rcf_3_delay + o_dlv_delay\n",
        "  )%>%\n",
        "  #Reordering the new total delay variables for clarity.\n",
        "  relocate(i1_total_delay, .after = i1_dlv_delay) %>%\n",
        "  relocate(i2_total_delay, .after = i2_dlv_delay) %>%\n",
        "  relocate(i3_total_delay, .after = i3_dlv_delay) %>%\n",
        "  relocate(o_total_delay,  .after = o_dlv_delay)%>%\n",
        "\n",
        "  #Now creating the final variable total_delay (sum of all legs)\n",
        "  mutate(total_delay = i1_total_delay + i2_total_delay + i3_total_delay + o_total_delay)\n",
        "\n",
        "delay_data_test %>%\n",
        "  select(contains(\"total_delay\"))%>%\n",
        "  head()"
      ],
      "metadata": {
        "id": "0etXS-tMI6DV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I created the variables *_total_delay* in the dataset \"delay_data_test\" rather than in the original delay_data because their construction requires replacing all NA values with 0. However, the original dataset \"delay_data\" contains NAs that may need to be preserved for the correlations analyses below. To avoid overwriting these missing values, I temporarily duplicated the data into \"delay_data_test\", applied the NA → 0 transformation only there, and computed the total delay variables in this safe copy. This test dataset will be used for the prediction models (Ridge, LASSO, Elastic Net) while we decide as a group when to process missing values in the final workflow.\n",
        "\n",
        "--------------------------------------------\n",
        "\n",
        "replace the NA's with 0!"
      ],
      "metadata": {
        "id": "NWENz1Mo4IPr"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YAnvJxeTnj9R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.4 Analysing the correlation between the variables**"
      ],
      "metadata": {
        "id": "8UjfWo1ftH1-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.4.1 Numerical variables"
      ],
      "metadata": {
        "id": "Z4UT1XletaEv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate correlations\n",
        "delay_cols <- grep(\"_delay$\", names(sqrt_delay_data), value = TRUE)\n",
        "delay_corr_data <- sqrt_delay_data[delay_cols]\n",
        "corr_matrix <- cor(delay_corr_data, use = \"pairwise.complete.obs\")\n",
        "\n",
        "# Convert the correlation matrix to long format for ggplot\n",
        "corr_df <- as.data.frame(corr_matrix)\n",
        "corr_df$Var1 <- rownames(corr_df)\n",
        "corr_long <- corr_df %>%\n",
        "  pivot_longer(-Var1, names_to = \"Var2\", values_to = \"correlation\")\n",
        "\n",
        "# Plot the heatmap\n",
        "ggplot(corr_long, aes(x = Var1, y = Var2, fill = correlation)) +\n",
        "  geom_tile() +\n",
        "  scale_fill_gradient2(low = \"blue\", high = \"red\", mid = \"white\",\n",
        "                       midpoint = 0, limit = c(-1, 1), space = \"Lab\",\n",
        "                       name=\"Correlation\") +\n",
        "  theme_minimal() +\n",
        "  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n",
        "  labs(title = \"Delay Column Correlation Heatmap\", x = \"\", y = \"\")"
      ],
      "metadata": {
        "id": "fOaYkLtNc0XM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "WqZH_bePEgY7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TO-DO**\n",
        "\n",
        "1. grey means too little observations\n",
        "2. negative correlation: if there is one delay, the following step will try to recover this\n",
        "3. positive correlation: positive correlation across different legs\n",
        "4. syncronisation of legs?"
      ],
      "metadata": {
        "id": "6lbzdNkSzNgW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.4.2 By airports"
      ],
      "metadata": {
        "id": "x_8dlMujthzT"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "09i2sFPiK6el"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gNM0KSY2nySY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "KzSIY7KKtDtt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Prediction**\n"
      ],
      "metadata": {
        "id": "pYgQK9JinnHl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preparing data for prediction\n",
        "In order to run prediction algorithms, we need to tackle missing data. As ellaborated in *section 2.1 Analysing the NA's*, we will replace all missing values with a 0."
      ],
      "metadata": {
        "id": "khNdDyr5rO13"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sqrt_delay_data <- sqrt_delay_data %>%\n",
        "  mutate(across(\n",
        "    .cols = c(where(is.numeric), contains(\"place\")), # numeric columns and those that contain a \"place\"\n",
        "    .fns  = ~ replace(.x, is.na(.x), 0)\n",
        "  ))"
      ],
      "metadata": {
        "id": "SUP249LIrbpq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sqrt_delay_data"
      ],
      "metadata": {
        "id": "cW98yE37b8Zo",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Team: please use the following data set:\n",
        "summary(sqrt_delay_data)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "aAOKANBVrPez"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I temporarily apply the square-root transformation to \"delay_data_test\" (instead of the original \"delay_data\") because this is the dataset where missing values were replaced with zeros and where the total_delay_* variables were created. Keeping all processing steps in the same dataset avoids mixing raw and transformed values and ensures that all delay variables—both individual delays and aggregated delays—are on the same scale before running Ridge, LASSO, and Elastic Net.\n",
        "\n",
        "This setup is temporary: after discussing with the group, we will adjust the code to apply the transformation directly on the final version of delay_data."
      ],
      "metadata": {
        "id": "q4Q_bITHfgv6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# creating our square root-function\n",
        "ssqrt <- function(x) {sign(x) * sqrt(abs(x))}\n",
        "\n",
        "# Vector of leg prefixes to match\n",
        "leg_prefixes <- c(\"i1\", \"i2\", \"i3\", \"o\")\n",
        "\n",
        "# Select columns\n",
        "cols_to_transform <- c(\n",
        "  grep(paste0(\"^((\", paste(leg_prefixes, collapse = \"|\"), \"))_.*delay$\"), names(delay_data_test), value = TRUE), # Select columns matching any of the leg prefixes\n",
        "  grep(\"^total_delay\", names(delay_data_test), value = TRUE) #Select the columns capturing the total_delay\n",
        ")\n",
        "\n",
        "# Create copies for transformations\n",
        "sqrt_delay_data_test <- delay_data_test\n",
        "\n",
        "# Apply ssqrt transform\n",
        "sqrt_delay_data_test[cols_to_transform] <- lapply(delay_data_test[cols_to_transform], function(col) ssqrt(col))\n",
        "\n",
        "sqrt_delay_data_test"
      ],
      "metadata": {
        "collapsed": true,
        "id": "fQ1CmGL4a9kz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary(sqrt_delay_data_test)"
      ],
      "metadata": {
        "id": "Uch-okHkeJE3",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8OlECMvIwr3v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.1 Classic Linear Regression"
      ],
      "metadata": {
        "id": "iqyelqCjnzc2"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JGNhbvoan3l1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.2 Linear Models"
      ],
      "metadata": {
        "id": "-TEpevFUn38G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.2.1 Setting Up the Data"
      ],
      "metadata": {
        "id": "33ldFoVCn63Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Define the response variable and features for our predictive model\n",
        "\n",
        "# Response variable (y): the total delay of each shipment\n",
        "y <- sqrt_delay_data_test$total_delay\n",
        "\n",
        "# Feature variables (X): all step-by-step delays and aggregated delays per leg\n",
        "features <- setdiff(grep(\"_delay$\", names(sqrt_delay_data_test), value = TRUE), \"total_delay\")\n",
        "\n",
        "#Subset the dataset to only keep the feature columns\n",
        "X <- sqrt_delay_data_test[, features]"
      ],
      "metadata": {
        "id": "Wkz5GSTcoO-d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Quick check\n",
        "dim(X)\n",
        "length(y)\n",
        "#Looks alright"
      ],
      "metadata": {
        "id": "FISOL5dy_nNY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting into train data and test data (70/30)\n",
        "\n",
        "set.seed(2025)  # ensure we get the same split every time\n",
        "\n",
        "# Randomly sample 70% of the row indices for the training set\n",
        "n <- nrow(sqrt_delay_data_test)\n",
        "train_indices <- sample(1:n, size = 0.7 * n)\n",
        "\n",
        "# Create training and test sets\n",
        "X_train <- X[train_indices, ]   # features for training\n",
        "X_test  <- X[-train_indices, ]  # features for testing\n",
        "\n",
        "y_train <- y[train_indices]     # response for training\n",
        "y_test  <- y[-train_indices]    # response for testing"
      ],
      "metadata": {
        "id": "oX6uw2h773be"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Quick check\n",
        "dim(X_train)\n",
        "length(y_train)\n",
        "dim(X_test)\n",
        "length(y_test)\n",
        "#Looks alright as well"
      ],
      "metadata": {
        "id": "FBVaW8kC_v46"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A standardization of the features is necessary in penalized regressions like Ridge, LASSO, or Elastic Net to ensure that the penalty applies fairly across all variables, regardless of their original scale.\n",
        "\n",
        "In our dataset, this is particularly important for two reasons:\n",
        "1) some delay variables have very negative values while others are more centered around zero, creating large differences in scale;\n",
        "2) some features, such as i1_total_delay, i2_total_delay, i3_total_delay, and o_total_delay, are sums of other delay variables, giving them naturally larger magnitudes.\n",
        "\n",
        "Standardizing all features puts them on a comparable scale, preventing variables with larger ranges from dominating the penalty and ensuring meaningful coefficient estimates."
      ],
      "metadata": {
        "id": "MCBnFgGpCjJS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute mean and standard deviation of each feature on the training set\n",
        "feature_means <- apply(X_train, 2, mean)\n",
        "feature_sds   <- apply(X_train, 2, sd)\n",
        "\n",
        "# Standardize training set\n",
        "X_train_scaled <- scale(X_train, center = feature_means, scale = feature_sds)\n",
        "\n",
        "# Standardize test set using the training set statistics\n",
        "X_test_scaled <- scale(X_test, center = feature_means, scale = feature_sds)"
      ],
      "metadata": {
        "id": "cm_RHq3T_xqu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.2.2 Ridge"
      ],
      "metadata": {
        "id": "ImSsVaAdGZvc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Using Ridge Model on the training data\n",
        "ridge_model <- glmnet(\n",
        "  x = X_train_scaled,\n",
        "  y = y_train,\n",
        "  alpha = 0 #alpha = 0 for the L2 penalty (sum of squares)\n",
        ")\n",
        "\n",
        "print(ridge_model)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "gtBA6FXGG2GO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot(ridge_model)\n",
        "abline(h=0)\n",
        "grid()"
      ],
      "metadata": {
        "id": "U8cr2rYJWJQu",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Finding the best lambda\n",
        "cv_ridge <- cv.glmnet(\n",
        "  x = X_train_scaled,\n",
        "  y = y_train,\n",
        "  alpha = 0\n",
        ")\n",
        "\n",
        "print(cv_ridge)"
      ],
      "metadata": {
        "id": "A6xDIUUVNBU-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot(cv_ridge)"
      ],
      "metadata": {
        "id": "7VsNReHnV2Do",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We obtain two relevant values of λ for the Ridge model.\n",
        "\n",
        "The optimal $\\lambda_{min}$ (lambda.min), which minimizes the mean squared error (MSE), is 5.44. This value provides the best predictive performance for $y$. At this relatively small $\\lambda$, the penalty imposed by Ridge—proportional to the sum of squared coefficients is weak, allowing the coefficients to remain relatively large. As a result, each feature retains an influence close to that of a classical OLS regression, with only slight shrinkage to prevent excessively extreme coefficients. \\\n",
        "While the model predicts well, it is somewhat more sensitive to small variations in the data because the coefficients are less constrained. Specifically, if some observations are atypical or contain errors, the small penalty allows the coefficients to fluctuate more in order to adapt to these points. Additionally, if features are highly correlated, some coefficients may take large values, and with a small $\\lambda$, Ridge does not shrink them sufficiently, leading to potential oscillations and reduced stability on new data.\n",
        "\n",
        "\n",
        "In contrast, the $\\lambda_{1se}$ value corresponding to one standard error above the optimal $\\lambda_{min}$ (lambda.1se) is 21.96. At this larger $\\lambda_{1se}$, the penalty is stronger, and the coefficients are more heavily shrunk towards zero. This reduces the influence of highly correlated or noisy variables and makes the model less sensitive to random fluctuations in the dataset. Consequently, if cross-validation were repeated on a different sample, the coefficients would be less likely to vary substantially, improving stability and robustness.\n",
        "\n",
        "\n",
        "For our case, the choice between these two $\\lambda$ values depends on the characteristics of our dataset. If we found strong correlations among the features or a significant number of atypical observations, the more regularized $\\lambda_{1se}$ (21.96) should be preferred. If correlations are moderate and the dataset does not contain extreme observations, using the $\\lambda_{min}$ that minimizes the MSE (5.44) is reasonable, as it provides better predictive accuracy."
      ],
      "metadata": {
        "id": "s7FXQ3oRTEC4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Extract the lambda.1se from cross-validation\n",
        "best_lambda_1se <- cv_ridge$lambda.1se\n",
        "best_lambda_1se\n",
        "# This should return 21.96\n",
        "\n",
        "# Predict on the test set using lambda.1se\n",
        "ridge_pred_1se <- predict(cv_ridge,\n",
        "                          s = best_lambda_1se,\n",
        "                          newx = X_test_scaled)\n",
        "\n",
        "#Compute the Mean Squared Error (MSE) on the test set\n",
        "mse_test_1se <- mean((y_test - ridge_pred_1se)^2)\n",
        "mse_test_1se\n"
      ],
      "metadata": {
        "id": "Xx_dR5UGUfTn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Extract the lambda.min from cross-validation\n",
        "best_lambda_min <- cv_ridge$lambda.min\n",
        "best_lambda_min\n",
        "# This should return 5.44\n",
        "\n",
        "# Predict on the test set using lambda.min\n",
        "ridge_pred_min <- predict(cv_ridge,\n",
        "                          s = best_lambda_min,\n",
        "                          newx = X_test_scaled)\n",
        "\n",
        "#Compute the Mean Squared Error (MSE) on the test set\n",
        "mse_test_min <- mean((y_test - ridge_pred_min)^2)\n",
        "mse_test_min\n"
      ],
      "metadata": {
        "id": "lO9v59hPXjZg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary(y_test)"
      ],
      "metadata": {
        "id": "ln-TbLOSYqmM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The comparison with the MSE obtained at $\\lambda_{min}$ (≈ 835) shows that the performance gap between the two models remains very small, on the order of 4–5%. In other words, using $\\lambda_{1se}$ provides a more regularized model—hence simpler and more stable with respect to data variations—while sacrificing only a very marginal amount of predictive accuracy. This suggests that the model tuned with $\\lambda_{1se}$ generalizes at least as well as, and potentially better than, the one based on $\\lambda_{min}$, despite the slight increase in test-set MSE.\n",
        "\n",
        "Importantly, these error levels remain coherent given the scale of the dependent variable: the test-set values of ($y$) span a wide range (from –167 to 745). A MSE around 800–900 corresponds to a Root Mean Squared Error of roughly 29 units, which is small relative to the total variation of the outcome variable. In this context, such error magnitudes are reasonable and indicate that the ridge model succeeds in capturing substantial structure in the data despite its inherent noise and dispersion.\n",
        "\n",
        "\n",
        "=> Need to check with the MSE of the OLS Regression part 3.1\n"
      ],
      "metadata": {
        "id": "dXR9iUezbVOQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert predictions to numeric\n",
        "ridge_pred_min_num <- as.numeric(predict(cv_ridge, s = cv_ridge$lambda.min, newx = X_test_scaled))\n",
        "ridge_pred_1se_num <- as.numeric(ridge_pred_1se)\n",
        "\n",
        "# Create a dataframe for plotting\n",
        "plot_data <- data.frame(\n",
        "  Observed = y_test,\n",
        "  Predicted_min = ridge_pred_min_num,\n",
        "  Predicted_1se = ridge_pred_1se_num\n",
        ")\n",
        "\n",
        "# Set up plotting area: 1 row, 2 columns\n",
        "par(mfrow = c(1,2))\n",
        "\n",
        "# Plot for lambda.min\n",
        "plot(plot_data$Observed, plot_data$Predicted_min,\n",
        "     xlab = \"Observed Values (Test Set)\",\n",
        "     ylab = \"Predicted Values (Ridge λ.min)\",\n",
        "     main = \"Observed vs Predicted – λ.min\",\n",
        "     pch = 19, col = \"blue\")\n",
        "abline(a = 0, b = 1, col = \"red\", lwd = 2)\n",
        "\n",
        "# Plot for lambda.1se\n",
        "plot(plot_data$Observed, plot_data$Predicted_1se,\n",
        "     xlab = \"Observed Values (Test Set)\",\n",
        "     ylab = \"Predicted Values (Ridge λ.1se)\",\n",
        "     main = \"Observed vs Predicted – λ.1se\",\n",
        "     pch = 19, col = \"green\")\n",
        "abline(a = 0, b = 1, col = \"red\", lwd = 2)\n",
        "\n",
        "# Reset plotting layout to default\n",
        "par(mfrow = c(1,1))"
      ],
      "metadata": {
        "id": "1Ko1LiVTdeg8",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Using Ridge Model on the training data\n",
        "ridge_model <- glmnet(\n",
        "  x = X_train,\n",
        "  y = y_train,\n",
        "  alpha = 0 #alpha = 0 for the L2 penalty (sum of squares)\n",
        ")\n",
        "\n",
        "print(ridge_model)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "VQlzsA8hBHmf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot(ridge_model)\n",
        "abline(h=0)\n",
        "grid()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "enYUoVEkBHmh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Finding the best lambda\n",
        "cv_ridge <- cv.glmnet(\n",
        "  x = as.matrix(X_train),\n",
        "  y = y_train,\n",
        "  alpha = 0\n",
        ")\n",
        "\n",
        "print(cv_ridge)"
      ],
      "metadata": {
        "id": "E3xLDRNZBHmh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot(cv_ridge)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "BNtRkVz4BHmh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We obtain two relevant values of λ for the Ridge model.\n",
        "\n",
        "The optimal $\\lambda_{min}$ (lambda.min), which minimizes the mean squared error (MSE), is 5.44. This value provides the best predictive performance for $y$. At this relatively small $\\lambda$, the penalty imposed by Ridge—proportional to the sum of squared coefficients is weak, allowing the coefficients to remain relatively large. As a result, each feature retains an influence close to that of a classical OLS regression, with only slight shrinkage to prevent excessively extreme coefficients. \\\n",
        "While the model predicts well, it is somewhat more sensitive to small variations in the data because the coefficients are less constrained. Specifically, if some observations are atypical or contain errors, the small penalty allows the coefficients to fluctuate more in order to adapt to these points. Additionally, if features are highly correlated, some coefficients may take large values, and with a small $\\lambda$, Ridge does not shrink them sufficiently, leading to potential oscillations and reduced stability on new data.\n",
        "\n",
        "\n",
        "In contrast, the $\\lambda_{1se}$ value corresponding to one standard error above the optimal $\\lambda_{min}$ (lambda.1se) is 21.96. At this larger $\\lambda_{1se}$, the penalty is stronger, and the coefficients are more heavily shrunk towards zero. This reduces the influence of highly correlated or noisy variables and makes the model less sensitive to random fluctuations in the dataset. Consequently, if cross-validation were repeated on a different sample, the coefficients would be less likely to vary substantially, improving stability and robustness.\n",
        "\n",
        "\n",
        "For our case, the choice between these two $\\lambda$ values depends on the characteristics of our dataset. If we found strong correlations among the features or a significant number of atypical observations, the more regularized $\\lambda_{1se}$ (21.96) should be preferred. If correlations are moderate and the dataset does not contain extreme observations, using the $\\lambda_{min}$ that minimizes the MSE (5.44) is reasonable, as it provides better predictive accuracy."
      ],
      "metadata": {
        "id": "7flvtdcYBHmh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_test_s <- X_test"
      ],
      "metadata": {
        "id": "CAgBV0pmCeze"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test_s[ , grepl(\"o\", names(X_test_s)) ] <- 0\n",
        "X_test_s"
      ],
      "metadata": {
        "id": "oE09Nn37C47m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Extract the lambda.1se from cross-validation\n",
        "best_lambda_1se <- cv_ridge$lambda.1se\n",
        "best_lambda_1se\n",
        "# This should return 21.96\n",
        "\n",
        "# Predict on the test set using lambda.1se\n",
        "ridge_pred_1se <- predict(cv_ridge,\n",
        "                          s = best_lambda_1se,\n",
        "                          newx = as.matrix(X_test_s))\n",
        "\n",
        "#Compute the Mean Squared Error (MSE) on the test set\n",
        "mse_test_1se <- mean((y_test - ridge_pred_1se)^2)\n",
        "mse_test_1se\n"
      ],
      "metadata": {
        "id": "btHKQRx-BHmi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ridge_pred_1se"
      ],
      "metadata": {
        "id": "q7RkDnQ-GPFB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test"
      ],
      "metadata": {
        "id": "856G-xs9Dx5k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Extract the lambda.min from cross-validation\n",
        "best_lambda_min <- cv_ridge$lambda.min\n",
        "best_lambda_min\n",
        "# This should return 5.44\n",
        "\n",
        "# Predict on the test set using lambda.min\n",
        "ridge_pred_min <- predict(cv_ridge,\n",
        "                          s = best_lambda_min,\n",
        "                          newx = as.matrix(X_test))\n",
        "\n",
        "#Compute the Mean Squared Error (MSE) on the test set\n",
        "mse_test_min <- mean((y_test - ridge_pred_min)^2)\n",
        "mse_test_min\n"
      ],
      "metadata": {
        "id": "_LVUJA29BHmi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary(y_test)"
      ],
      "metadata": {
        "id": "pgoNkjtfBHmi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The comparison with the MSE obtained at $\\lambda_{min}$ (≈ 835) shows that the performance gap between the two models remains very small, on the order of 4–5%. In other words, using $\\lambda_{1se}$ provides a more regularized model—hence simpler and more stable with respect to data variations—while sacrificing only a very marginal amount of predictive accuracy. This suggests that the model tuned with $\\lambda_{1se}$ generalizes at least as well as, and potentially better than, the one based on $\\lambda_{min}$, despite the slight increase in test-set MSE.\n",
        "\n",
        "Importantly, these error levels remain coherent given the scale of the dependent variable: the test-set values of ($y$) span a wide range (from –167 to 745). A MSE around 800–900 corresponds to a Root Mean Squared Error of roughly 29 units, which is small relative to the total variation of the outcome variable. In this context, such error magnitudes are reasonable and indicate that the ridge model succeeds in capturing substantial structure in the data despite its inherent noise and dispersion.\n",
        "\n",
        "\n",
        "=> Need to check with the MSE of the OLS Regression part 3.1\n"
      ],
      "metadata": {
        "id": "tl_RA4WQBHmi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert predictions to numeric\n",
        "ridge_pred_min_num <- as.numeric(predict(cv_ridge, s = cv_ridge$lambda.min, newx = X_test))\n",
        "ridge_pred_1se_num <- as.numeric(ridge_pred_1se)\n",
        "\n",
        "# Create a dataframe for plotting\n",
        "plot_data <- data.frame(\n",
        "  Observed = y_test,\n",
        "  Predicted_min = ridge_pred_min_num,\n",
        "  Predicted_1se = ridge_pred_1se_num\n",
        ")\n",
        "\n",
        "# Set up plotting area: 1 row, 2 columns\n",
        "par(mfrow = c(1,2))\n",
        "\n",
        "# Plot for lambda.min\n",
        "plot(plot_data$Observed, plot_data$Predicted_min,\n",
        "     xlab = \"Observed Values (Test Set)\",\n",
        "     ylab = \"Predicted Values (Ridge λ.min)\",\n",
        "     main = \"Observed vs Predicted – λ.min\",\n",
        "     pch = 19, col = \"blue\")\n",
        "abline(a = 0, b = 1, col = \"red\", lwd = 2)\n",
        "\n",
        "# Plot for lambda.1se\n",
        "plot(plot_data$Observed, plot_data$Predicted_1se,\n",
        "     xlab = \"Observed Values (Test Set)\",\n",
        "     ylab = \"Predicted Values (Ridge λ.1se)\",\n",
        "     main = \"Observed vs Predicted – λ.1se\",\n",
        "     pch = 19, col = \"green\")\n",
        "abline(a = 0, b = 1, col = \"red\", lwd = 2)\n",
        "\n",
        "# Reset plotting layout to default\n",
        "par(mfrow = c(1,1))"
      ],
      "metadata": {
        "collapsed": true,
        "id": "SelWxkHFBHmi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.2.3 Lasso"
      ],
      "metadata": {
        "id": "76rySxHUn9Kr"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9R13Er-XoOib"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.2.4 Elastic Net"
      ],
      "metadata": {
        "id": "Z4cs0VoWoADT"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7neAInIboOEr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.3 Non-linear Models"
      ],
      "metadata": {
        "id": "9lL64rfVoCfo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.3.1 K-nearest neighbours\n"
      ],
      "metadata": {
        "id": "sUlYShfAoGf2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# partitioning scaled train data from above into train and validation data\n",
        "v <- nrow(X_train_scaled)\n",
        "val_indices <- sample(1:v, size = 0.3 * v) # taking 30% of our scaled test data to use for validation\n",
        "# this is just a choice I'm making for now so that I can use the same test split\n",
        "# as the Ridge model to compare model performance on the same test split\n",
        "# We should discuss this though!\n",
        "\n",
        "# creating train and validation sets\n",
        "\n",
        "X_train_scaled_knn <- X_train_scaled[-val_indices,] # features for training\n",
        "X_val_scaled_knn <- X_train_scaled[val_indices,] # features for validation\n",
        "\n",
        "# response variables\n",
        "y_train_knn <- y_train[-val_indices] # response for training\n",
        "y_val_knn <- y_train[val_indices] # response for validation\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "TpJ7o0TcoNkY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Quick Check\n",
        "dim(X_train_scaled_knn)\n",
        "length(y_train_knn)\n",
        "dim(X_val_scaled_knn)\n",
        "length(y_val_knn)"
      ],
      "metadata": {
        "id": "ic3cfKMAk6G4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# KNN set up\n",
        "K <- 50 # setting a max K of 50, discuss if we want to go higher/lower\n",
        "\n",
        "# Creating empty (for now) vectors to store MSE for each K\n",
        "train_MSE <- rep(NA, K) # will hold MSE for each K on train data\n",
        "val_MSE <- rep(NA, K) # will hold MSE for each K on val data"
      ],
      "metadata": {
        "id": "tGRJBJZcld_A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# KNN train and validation loop\n",
        "for (k in 1:K){ # iterates from 1 to max value of K\n",
        "  # kNN with training data\n",
        "  kNN_train <- knn.reg(train = X_train_scaled_knn,\n",
        "                 test = X_train_scaled_knn,\n",
        "                 y = y_train_knn,\n",
        "                 k = k)\n",
        "  # calculating train MSE for each K and storing each K's MSE in vector\n",
        "  # train_MSE[k] <- mean(( - y_train_knn)^2)\n",
        "\n",
        "  # kNN with validation data\n",
        "  kNN_val  <- knn.reg(train = X_train_scaled_knn,\n",
        "                test = X_val_scaled_knn,\n",
        "                y = y_train_knn,\n",
        "                k = k)\n",
        "  # calculating validation MSE for each K and storing each K's MSE in vector\n",
        "  # val_MSE[k]  <- mean((knn_val-y_val_knn)^2)\n",
        "}"
      ],
      "metadata": {
        "id": "7GH8XVApmt0J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluating values of K\n",
        "# comparison between training performance and validation performance\n",
        "plot(1:K,val_MSE, t ='l',ylim=c(min(val_MSE),1),col='darkblue',lwd=2)\n",
        "lines(1:K,train_MSE, col='firebrick',lwd=2)\n",
        "grid()\n",
        "legend('top',c('Val. MSE','Train MSE'), col=c('darkblue','firebrick'),lty=1,lwd=3)\n",
        "# numeric calculation of best K value\n",
        "k_star <- (1:K)[which.min(val_MSE)]\n",
        "# printing best model's MSE and K value\n",
        "sprintf('Best model minimized MSE to %.4f with %.s neighbors', min(val_MSE), k_star)"
      ],
      "metadata": {
        "id": "HLuO0ogfw-s8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# KNN on test data using best K value from training/validation\n",
        "kNN_test <- knn.reg(train = X_train_scaled_knn,\n",
        "                    test = X_test_scaled,\n",
        "                    y = X_train_scaled_knn,\n",
        "                    k = k_star)"
      ],
      "metadata": {
        "id": "pj9x71nXwXHV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.3.2 Random forest"
      ],
      "metadata": {
        "id": "i4WOWvGUoJZi"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Z2Bnm6nIoNGL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}